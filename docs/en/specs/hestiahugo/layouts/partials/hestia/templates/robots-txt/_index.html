+++
draft = false
layout = "single"
+++

<section id="introduction">
	<h1>{{% param "title" %}}</h1>
	<p>
		{{%/* param "description" */%}}
	</p>
</section>




<section id="objective">
	<h2>Objective</h2>
	<p>
		HestiaHUGO's <code>robots.txt</code> template rendering
		partial function aims to render a consistent single
		<code>robots.txt</code> data file as output. It utilizes Hugo's
		<a href="https://gohugo.io/templates/robots/">Robots.txt File
		Rendering</a> feature alongside HestiaHUGO data processing
		partial functions.
	</p>
</section>




<section id="algorithm">
	<h2>Algorithm</h2>
	<p>
		Here are a summary of how this template function works.
	</p>


	<section id="input">
		<h3>Input</h3>
		<p>
			This template function takes all the data files inside
			<code>data/Hestia/Robots/</code> directory, with the
			filename as the <code>User-Agent:</code> value. All the
			data are pre-processed and whitespace clean up before
			use. The field names are case-insensitive.
			<br/><br/>
			Note that a special <code>{{ .BaseURL }}</code>
			placeholder is made availabe for the data file to
			construct absolute URL from a given relative URL
			whenever required. <b>It shall be replaced into your
			given Base URL to Hugo</b>.
		</p>
	</section>


	<section id="processing">
		<h3>Processing</h3>
		<p>
			This partial function operates in the following steps:
		</p>
		<ol>
			<li><p>
				Parse all the data files in from the data
				source directory.
			</p></li>
			<li><p>
				For each dataset, process:
				<ul>
					<li><p>
						<code>.Name</code> with
						<code>User-Agent:</code> value.
					</p></li>
					<li><p>
						<code>.Sitemaps</code> with
						<code>Sitemap:</code> values.
					</p></li>
					<li><p>
						<code>.Allow</code> with
						<code>Allow:</code> values.
					</p></li>
					<li><p>
						<code>.Disallow</code> with
						<code>Disallow:</code> values.
					</p></li>
					<li><p>
						<code>.Crawl-delay</code> with
						<code>Crawl-delay:</code>
						value.
					</p></li>
				</ul>
				with <code>{{ .BaseURL }}</code> replacement,
				accurate field name generations, and whitespace
				cleaning.
			</p></li>
			<li><p>
				Make <code>.Sitemaps</code> list unique.
			</p></li>
			<li><p>
				Render the processed data structure complying
				to <code>robots.txt</code>.
			</p></li>
		</ol>
	</section>


	<section id="output">
		<h2>Output</h2>
		<p>
			The output is the actual <code>robots.txt</code> file's
			content complying to
			<a href="https://developers.google.com/search/docs/advanced/robots/intro">
			Google Search Central Specification</a>. Some notable
			fixtures are:
		</p>
		<ol>
			<li><p>
				<code>.Sitemaps</code> are always unique and
				filled first.
			</p></li>
			<li><p>
				<code>User-Agent: *</code> is always rendered
				first from the list.
			</p></li>
			<li><p>
				All other <code>User-Agent:</code> are rendered
				in an unordered manner after the above.
			</p></li>
		</ol>
	</section>
</section>




<section id="epilogue">
	<h2>Epilogue</h2>
	<p>
		That's all for {{% param "title" %}}
	</p>
</section>
